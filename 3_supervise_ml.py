# -*- coding: utf-8 -*-
"""3. Supervise ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kCHxGKhHx4AyP7E03oVbPM6A_Z5x33SW

## 1. Combining data frames
file 1: soil_nutrient_data.xlsx
file 2: abundance_order_level.csv


you HAVE to connect your google drive and set the working directory as our shared Erdos_project folder. Otherwise you won't be able to access the files.

Dataframe has been merged. You can find the file in our shared directory called: combined_soil_otu_data.csv
"""

# Commented out IPython magic to ensure Python compatibility.
# Need to link google drive and jupyter notebook as we did in the first workbook
# link your google drive. You'll have to do this everytime you open up the notebook or reset the run time.

from google.colab import drive

drive.mount('/content/gdrive/', force_remount=True)

# the easiest way to access a shared google folder is to add the shared folder as a short cut into your drive
# find "Erdos_project" folder that Kim shared, right click, click on organize, then click on add shortcut

# %cd gdrive/MyDrive/Erdos_project

# if you get this error: [Errno 107] Transport endpoint is not connected: 'gdrive/MyDrive' /content/gdrive/MyDrive
# then go to "Runtime" at the top, and click "restart runtime"

# load the data in
# OTU table: filtered_otu_table.tsv
# soil nutrient data: soil_nutrient_data.xlsx
import os

# get the current working directory
current_working_directory = os.getcwd()

# print output to the console
print(current_working_directory)


import pandas as pd

# read in data file

otu = pd.read_csv('abundance_order_level_transpose.csv', index_col=False)
otu.head()

soil = pd.read_excel(r"soil_nutrient_data.xlsx")
print(soil)

# removing these columns from soil: temperature [Tm (C)], rainfall [Precip. (in)], and electrical current [EC (uhmo/cm)]
# List of columns to remove
columns_to_remove = ['Samples','Precip. (in)', 'Tm (C)', 'EC (uhmo/cm)']

# Remove specified columns
new_soil = soil.drop(columns=columns_to_remove)

new_soil.head()

# combine the two table: otu, new_soil

merged_df = pd.merge(new_soil, otu, how='inner', left_on='Location', right_on='Unnamed: 0')

merged_df.head()

merged_df = merged_df.rename(columns={'Unnamed: 0': 'Samples'})

merged_df.head()

merged_df["S (ppm)"]

merged_df.to_csv('combined_soil_otu_data.csv', index=True)

"""## 2. K-nearest neighbor
load in file combined_soil_otu_data.csv

from data science bootcamp notebook: 10. Regression Version of Classification Algorithms
ùëò nearest neighbors regression can be performed with sklearn's KNeighborsRegressor model object, https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html.
"""

df = pd.read_csv('combined_soil_otu_data.csv', index_col=False)
df.head()

#df.set_index('Location', inplace=True)

#df.head()

column_names = df.columns.tolist()
print(column_names)

df["S (ppm)"]

del df['Unnamed: 0']
del df['Location']
del df['Samples']
df.head()

#df['Yield (bu/acre)'] = pd.to_numeric(df['Yield (bu/acre)'], errors='coerce')

import pandas as pd
import numpy as nnp

import matplotlib.pyplot as plt
from seaborn import set_style

set_style("whitegrid")

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

print(df)

# Select features (X) and target variable (y)
X = df.drop('Yield (bu/acre)', axis=1)  # Adjust column names as needed
#need to drop location, samples
y = df['Yield (bu/acre)']

print(y)

print(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42) #70/30 split
#X_train are the features, y_train is what we are predicting

print(X_train)

print(y_train)



# Create a KNN regressor with k=3 and weights='distance' for inverse distance weighting
# I tested k = 4 and k = 2 below, but k=3 has given the best results
knn_regressor = KNeighborsRegressor(n_neighbors=3, weights='distance')

# Fit the model to the training data
knn_regressor.fit(X_train, y_train)

# Make predictions on the test data
y_pred = knn_regressor.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# mean squared error was 141.26 here, which is pretty high. The lower the MSE, the better the performance

# Create a KNN regressor with k=3 and weights='distance' for inverse distance weighting
# I tested k = 4 and k = 2 below, but k=3 has given the best results
# trying weight = 'uniform', which weighs all points equally. 'distance' weighs all points by the inverse of their distance
# closer neighbors will have more influence than neighbors far away
knn_regressor = KNeighborsRegressor(n_neighbors=3, weights='uniform')

# Fit the model to the training data
knn_regressor.fit(X_train, y_train)

# Make predictions on the test data
y_pred = knn_regressor.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# mean squared error was 138.885 here, which is pretty high. The lower the MSE, the better the performance

# Okay, so doing uniform distance improves the mse slighty

# Plot the results
plt.scatter(X_test.index, y_test, color='black', label='Actual')
plt.scatter(X_test.index, y_pred, color='red', label='Predicted')
plt.title('K-Nearest Neighbors Linear Regression')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable')
plt.legend()
plt.show()

# DON'T RUN THESE UNLESS YOU WANT TO SEE WHAT CHANGING THE K VALUE DOES
# Create a KNN regressor with k=3 and weights='distance' for inverse distance weighting
knn_regressor = KNeighborsRegressor(n_neighbors=4, weights='distance')

# Fit the model to the training data
knn_regressor.fit(X_train, y_train)

# Make predictions on the test data
y_pred = knn_regressor.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# mean squared error was 241.6 here, which is pretty high. The lower the MSE, the better the performance

# DON'T RUN THESE UNLESS YOU WANT TO SEE WHAT CHANGING THE K VALUE DOES
# Create a KNN regressor with k=2 and weights='distance' for inverse distance weighting
knn_regressor = KNeighborsRegressor(n_neighbors=2, weights='distance')

# Fit the model to the training data
knn_regressor.fit(X_train, y_train)

# Make predictions on the test data
y_pred = knn_regressor.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# mean squared error was 255.07 here, which is pretty high. The lower the MSE, the better the performance

# trying another way to run KNN
# this code is from 10. Regression Version of Classification Algorithms

from sklearn.model_selection import train_test_split

# Use pandas to import the data
# use df from above

df_train, df_test = train_test_split(df.copy(),
                                            shuffle=True,
                                            random_state=403,
                                            test_size=.3)

df.head()

# This is difficult because we are using one feature to predict
# yeild. We are using 48 different features to predict yeild.
# Each figure has values of different expected ranges.
# i.e. pH can range from 0 - 14 while OM % can range from 0 - 100%
# while abunance of taxa can range from 0 to hundreds/thousands
# first make a figure
plt.figure(figsize = (6,6))

# plt.scatter plots
# you can change what the y axis is to see how each feature is related to yeild
plt.scatter(df['Yield (bu/acre)'], df['Tepidisphaerales'])

# Always good practice to label well when
# presenting a figure to others
# place an xlabel
plt.xlabel("Yield", fontsize =12)

# place a ylabel
plt.ylabel("Tepidisphaerales", fontsize = 12)

# type this to show the plot
plt.show()

# RF regression with OTU and soil nutrients

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# data splitting based on what Kayla did and feature matrix preparation (X_train, X_test, y_train, y_test)

# Linear Regression
linear_regressor = LinearRegression()
linear_regressor.fit(X_train, y_train)
y_pred_linear = linear_regressor.predict(X_test)
mse_linear = mean_squared_error(y_test, y_pred_linear)
print(f'Linear Regression Mean Squared Error: {mse_linear}')

# Random Forest Regression
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor.fit(X_train, y_train)
y_pred_rf = rf_regressor.predict(X_test)
mse_rf = mean_squared_error(y_test, y_pred_rf)
print(f'Random Forest Regression Mean Squared Error: {mse_rf}')

# Support Vector Regression
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

svr_regressor = SVR(kernel='linear')
svr_regressor.fit(X_train_scaled, y_train)
y_pred_svr = svr_regressor.predict(X_test_scaled)
mse_svr = mean_squared_error(y_test, y_pred_svr)
print(f'Support Vector Regression Mean Squared Error: {mse_svr}')

# Plot the results
plt.scatter(X_test.index, y_test, color='black', label='Actual')
plt.scatter(X_test.index, y_pred_linear, color='blue', label='Linear Regression')
plt.scatter(X_test.index, y_pred_rf, color='green', label='Random Forest Regression')
plt.scatter(X_test.index, y_pred_svr, color='red', label='Support Vector Regression')
plt.title('Regression Models Comparison')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable')
plt.legend()
plt.show()

"""# What does the model look like if we only use the soil nutrient data?"""

# use table
soil = pd.read_excel(r"soil_nutrient_data.xlsx")
print(soil)

# removing these columns from soil: temperature [Tm (C)], rainfall [Precip. (in)], and electrical current [EC (uhmo/cm)]
# List of columns to remove
columns_to_remove = ['Samples','Precip. (in)', 'Tm (C)', 'EC (uhmo/cm)']

# Remove specified columns
new_soil = soil.drop(columns=columns_to_remove)

print(new_soil)

del new_soil['Location']
print(new_soil)

# Normalize the dataframe so that all values are on the same scale (ie 0 - 1 , not 1 - 100, .1- .9, etc.)
from sklearn.preprocessing import MinMaxScaler

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Normalize all columns in the DataFrame
soil_normalized = pd.DataFrame(scaler.fit_transform(new_soil), columns=new_soil.columns)

# Display the DataFrame after normalization
print("\nDataFrame after normalization:")
print(soil_normalized)

# correlation matrix
correlation_matrix = soil_normalized.corr()
print(correlation_matrix)

# Select features (X) and target variable (y)
X = soil_normalized.drop('Yield (bu/acre)', axis=1)  # Adjust column names as needed
#need to drop location, samples
y = soil_normalized['Yield (bu/acre)']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42) #70/30 split
#X_train are the features, y_train is what we are predicting

## import knnr
from sklearn.neighbors import KNeighborsRegressor

## import LinearRegression
from sklearn.linear_model import LinearRegression

# Create a KNN regressor with k=3 and weights='distance' for inverse distance weighting

knn_regressor = KNeighborsRegressor(n_neighbors=3, weights='distance')

# Fit the model to the training data
knn_regressor.fit(X_train, y_train)

# Make predictions on the test data
y_pred = knn_regressor.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# mean squared error was 141.26 here, which is pretty high. The lower the MSE, the better the performance


# mean squared error for running microbes and soil was 138.885 at the lowest
# which is pretty high and indicates poor performance

# when I removed the microbes, the mse is ALOT lower, 0.256

# Plot the results
plt.scatter(X_test.index, y_test, color='black', label='Actual')
plt.scatter(X_test.index, y_pred, color='red', label='Predicted')
plt.title('K-Nearest Neighbors Linear Regression')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable')
plt.legend()
plt.show()

# Create a KNN regressor with k=5 and weights='uniform' for inverse distance weighting
# I don't think we should go over k=5 since we have 8 in our training data. We don't want to overfit.

knn_regressor = KNeighborsRegressor(n_neighbors=5, weights='distance')

# Fit the model to the training data
knn_regressor.fit(X_train, y_train)

# Make predictions on the test data
y_pred = knn_regressor.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# mean squared error was 141.26 here, which is pretty high. The lower the MSE, the better the performance


# mean squared error for running microbes and soil was 138.885 at the lowest
# which is pretty high and indicates poor performance

# when I removed the microbes, the mse is ALOT lower, 0.256
# slight increase with k = 5

# Plot the results
plt.scatter(X_test.index, y_test, color='black', label='Actual')
plt.scatter(X_test.index, y_pred, color='red', label='Predicted')
plt.title('K-Nearest Neighbors Linear Regression')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable')
plt.legend()
plt.show()

"""K nearest neighbor is not typically used for feature importance, so I won't be doing that. I think random forest regression will be able to identify feature importance.

side note for myself: AUC are only used for classification problems

# K Nearest Neighbor Classification - predict what state crops are in based on soil nutrients
why this would be helpful, I don't know, but I want to see what a classification output would be
"""

"""#Random Forest Regression. Keep in mind, this is based on soil nutrients only because I most recently ran the X_train, Y_train values on soil nutrient data only"""

# WITH full table otu + soil nutrients
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# data splitting based on what Kayla did and feature matrix preparation (X_train, X_test, y_train, y_test)

# Linear Regression
linear_regressor = LinearRegression()
linear_regressor.fit(X_train, y_train)
y_pred_linear = linear_regressor.predict(X_test)
mse_linear = mean_squared_error(y_test, y_pred_linear)
print(f'Linear Regression Mean Squared Error: {mse_linear}')

# Random Forest Regression
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor.fit(X_train, y_train)
y_pred_rf = rf_regressor.predict(X_test)
mse_rf = mean_squared_error(y_test, y_pred_rf)
print(f'Random Forest Regression Mean Squared Error: {mse_rf}')

# Support Vector Regression
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

svr_regressor = SVR(kernel='linear')
svr_regressor.fit(X_train_scaled, y_train)
y_pred_svr = svr_regressor.predict(X_test_scaled)
mse_svr = mean_squared_error(y_test, y_pred_svr)
print(f'Support Vector Regression Mean Squared Error: {mse_svr}')

# Plot the results
plt.scatter(X_test.index, y_test, color='black', label='Actual')
plt.scatter(X_test.index, y_pred_linear, color='blue', label='Linear Regression')
plt.scatter(X_test.index, y_pred_rf, color='green', label='Random Forest Regression')
plt.scatter(X_test.index, y_pred_svr, color='red', label='Support Vector Regression')
plt.title('Regression Models Comparison without Microbiome Data')
plt.xlabel('Sample Index')
plt.ylabel('Target Variable')
plt.legend()
plt.show()

"""#Feature Importance"""

print(f'Linear Regression Coefficients: {linear_coefficients}')
rf_feature_importances = rf_regressor.feature_importances_
print(f'Random Forest Feature Importances: {rf_feature_importances}')

plt.figure(figsize=(10, 6))
plt.bar(range(len(linear_coefficients)), linear_coefficients, tick_label=X_train.columns)
plt.title('Linear Regression Feature Importance')
plt.xlabel('Feature')
plt.ylabel('Coefficient Value')
plt.show()

rf_feature_importances = rf_regressor.feature_importances_

# Get indices that would sort the feature importances
sorted_indices = rf_feature_importances.argsort()[::-1]

# Sort features
sorted_importances = rf_feature_importances[sorted_indices]
sorted_feature_labels = X_train.columns[sorted_indices]

# Plotting feature importance for Random Forest
plt.figure(figsize=(10, 6))
plt.bar(range(len(sorted_importances)), sorted_importances, tick_label=sorted_feature_labels)

# Rotate x-axis labels
plt.xticks(rotation=45, ha="right")

plt.title('Random Forest Feature Importance (Sorted)')
plt.xlabel('Feature')
plt.ylabel('Importance Score')
plt.tight_layout()  # Optional, to ensure everything fits in the figure
plt.show()

pip install shap
import shap

explainer = shap.TreeExplainer(rf_regressor)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test)
